{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd /root/Development/diffusers/examples/dreambooth\n",
    "\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install notebook\n",
    "\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install -e .\n",
    "cd ..\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "pip install safetensors\n",
    "\n",
    "accelerate config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git lfs install\n",
    "\n",
    "git clone https://huggingface.co/CompVis/stable-diffusion-v1-4\n",
    "git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
    "git clone https://huggingface.co/stabilityai/stable-diffusion-2-1-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person_ddim.git\n",
    "# git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_1_ddim_step.git\n",
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_euler\n",
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person-photographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./dog\"\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=local_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"CompVis/stable-diffusion-v1-4\"\n",
    "# MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "# MODEL_NAME = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "# MODEL_NAME = \"stabilityai/stable-diffusion-2-1\"\n",
    "# MODEL_NAME = \"BAAI/AltDiffusion-m9\"\n",
    "# MODEL_NAME = \"BAAI/AltDiffusion\"\n",
    "\n",
    "MODEL_NAME = \"stable-diffusion-v1-5\"\n",
    "# MODEL_NAME = \"stable-diffusion-2-1-base\"\n",
    "MODEL_PATH = f\"training_models/{MODEL_NAME}\"\n",
    "\n",
    "# INSTANCE_NAME = \"dog\"\n",
    "# INSTANCE_NAME = \"rabbit_toy\"\n",
    "# INSTANCE_NAME = \"gabrieltorcat\"\n",
    "# INSTANCE_NAME = \"gabrieltorcat2\"\n",
    "INSTANCE_NAME = \"gabrieltorcat_512\"\n",
    "INSTANCE_DIR = f\"training_images/{INSTANCE_NAME}\"\n",
    "# CLASS_DIR = \"dog\"\n",
    "# CLASS_DIR = \"toy\"\n",
    "CLASS_DIR = \"man\"\n",
    "# CLASS_DIR = \"person\"\n",
    "# CLASS_DIR = \"Stable-Diffusion-Regularization-Images-man_1_ddim_step/man_1_ddim_step\"\n",
    "# CLASS_DIR = \"Stable-Diffusion-Regularization-Images-person_ddim/person_ddim\"\n",
    "CLASS_DIR = f\"regularization_images/{MODEL_NAME}/{CLASS_DIR}\"\n",
    "OUTPUT_DIR = f\"outputs/{MODEL_NAME}/{INSTANCE_NAME}\"\n",
    "\n",
    "# PROMPT_TOKEN = \"sks\"\n",
    "# PROMPT_TOKEN = \"dbDog\"\n",
    "# PROMPT_TOKEN = \"dbRabbit\"\n",
    "PROMPT_TOKEN = \"gabrieltorcat\"\n",
    "# CLASS_TOKEN = \"dog\"\n",
    "# CLASS_TOKEN = \"toy\"\n",
    "CLASS_TOKEN = \"man\"\n",
    "# CLASS_TOKEN = \"person\"\n",
    "INSTANCE_PROMPT = f\"a photo of {PROMPT_TOKEN} {CLASS_TOKEN}\"\n",
    "# VALIDATION_PROMPT = f\"an oil painting of {PROMPT_TOKEN} {CLASS_TOKEN} sitting next to a wooden window reading a book, by Vincent Van Gogh\"\n",
    "VALIDATION_PROMPT = (\n",
    "    # f\"an oil painting of {PROMPT_TOKEN} {CLASS_TOKEN}, by Vincent Van Gogh\"\n",
    "    f\"a photo of {PROMPT_TOKEN} {CLASS_TOKEN} riding a bike in New York city\"\n",
    ")\n",
    "CLASS_PROMPT = f\"a photo of {CLASS_TOKEN}\"\n",
    "\n",
    "RESOLUTION = 512\n",
    "# RESOLUTION = 768\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "# TRAIN_BATCH_SIZE = 2\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "# GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "# LEARNING_RATE = 5e-6\n",
    "# LEARNING_RATE = 2e-6\n",
    "# LEARNING_RATE = 1e-6\n",
    "# LEARNING_RATE = 9e-7\n",
    "LEARNING_RATE = 8e-7\n",
    "\n",
    "# MAX_TRAIN_STEPS = 400\n",
    "# MAX_TRAIN_STEPS = 800\n",
    "# MAX_TRAIN_STEPS = 1200\n",
    "MAX_TRAIN_STEPS = 1600\n",
    "\n",
    "# NUM_CLASS_IMAGES = 50\n",
    "# NUM_CLASS_IMAGES = 100\n",
    "# NUM_CLASS_IMAGES = 300\n",
    "NUM_CLASS_IMAGES = 500\n",
    "# NUM_CLASS_IMAGES = 1500\n",
    "SAMPLE_BATCH_SIZE = 1\n",
    "# SAMPLE_BATCH_SIZE = 2\n",
    "\n",
    "CHECKPOINTING_STEPS = 200\n",
    "# CHECKPOINTING_STEPS = 300\n",
    "# CHECKPOINTING_STEPS = 400\n",
    "\n",
    "HUB_TOKEN = \"\"\n",
    "\n",
    "# NUM_CPU_THREADS_PER_PROCESS = 16\n",
    "NUM_CPU_THREADS_PER_PROCESS = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "accelerate launch \\\n",
    "  \n",
    "  --num_cpu_threads_per_process=$NUM_CPU_THREADS_PER_PROCESS \\\n",
    "  \n",
    "  train_dreambooth.py \\\n",
    "  \n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\" \\\n",
    "\n",
    "  --train_text_encoder \\\n",
    "\n",
    "# bitsandbytes\n",
    "#   --use_8bit_adam \\\n",
    "#   --gradient_checkpointing \\\n",
    "\n",
    "# xformers\n",
    "#   --enable_xformers_memory_efficient_attention \\\n",
    "#   --set_grads_to_none \\\n",
    "\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  # --resume_from_checkpoint=\"checkpoint-1500\" \\\n",
    "  --resume_from_checkpoint=\"latest\" \\\n",
    "\n",
    "  # --push_to_hub \\\n",
    "  # --hub_token=$HUB_TOKEN \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate launch \\\n",
    "  --num_cpu_threads_per_process=$NUM_CPU_THREADS_PER_PROCESS \\\n",
    "  train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\" \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  --resume_from_checkpoint=\"latest\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to stable diffusion format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python ../../scripts/convert_diffusers_to_original_stable_diffusion.py \\\n",
    "    --model_path=\"$OUTPUT_DIR\" \\\n",
    "    --checkpoint_path=\"$OUTPUT_DIR/sd_weights.ckpt\" \\\n",
    "    --half \\\n",
    "    # --use_safetensors \\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir OUTPUT_DIR + \"/logs/dreambooth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir $OUTPUT_DIR\"/logs/dreambooth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from diffusers import (\n",
    "    DiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    StableDiffusionPipeline,\n",
    "    DDIMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "prompts = [\n",
    "    INSTANCE_PROMPT,\n",
    "    VALIDATION_PROMPT,\n",
    "    INSTANCE_PROMPT + \" on top of mount fuji\",\n",
    "    INSTANCE_PROMPT + \" shaking hands with barack obama\",\n",
    "    INSTANCE_PROMPT + \" in front of the eiffel tower\",\n",
    "    INSTANCE_PROMPT + \" as an astronaut\",\n",
    "    INSTANCE_PROMPT + \" as a mecha robot\",\n",
    "    INSTANCE_PROMPT + \" shaking hands with emmanuel macron\",\n",
    "    INSTANCE_PROMPT + \" in a ramen bowl\",\n",
    "    INSTANCE_PROMPT + \" as neo fighting morpheus from the movie the matrix\",\n",
    "    INSTANCE_PROMPT + \" doing a handstand\",\n",
    "    INSTANCE_PROMPT + \" as a fighter jet pilot\",\n",
    "    INSTANCE_PROMPT + \" by the ocean\",\n",
    "    INSTANCE_PROMPT + \" holding the football world cup\",\n",
    "    INSTANCE_PROMPT + \" as mad max from mad max fury road\",\n",
    "    INSTANCE_PROMPT + \" as the joker from the dark night\",\n",
    "    INSTANCE_PROMPT + \" as the terminator\",\n",
    "    INSTANCE_PROMPT + \" as a Na'vi from the movie Avatar\",\n",
    "    INSTANCE_PROMPT + \" as a hobbit\",\n",
    "    INSTANCE_PROMPT + \" as an olympic athlete\",\n",
    "    INSTANCE_PROMPT + \" as a cowboy\",\n",
    "    INSTANCE_PROMPT + \" as a pirate\",\n",
    "]\n",
    "\n",
    "CHECKPOINTS = [\n",
    "    200,\n",
    "    400,\n",
    "    600,\n",
    "    800,\n",
    "    1000,\n",
    "    1200,\n",
    "    1400,\n",
    "    1600,\n",
    "    None,\n",
    "]\n",
    "\n",
    "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
    "model_id = MODEL_PATH\n",
    "\n",
    "print(f\"Model = {model_id}\")\n",
    "\n",
    "for checkpoint in CHECKPOINTS:\n",
    "    print(f\"Checkpoint = {checkpoint}\")\n",
    "\n",
    "    if checkpoint:\n",
    "        image_dir = (\n",
    "            OUTPUT_DIR + \"/test_output_images\" + \"/checkpoint-\" + str(checkpoint)\n",
    "        )\n",
    "\n",
    "        if image_dir is not None:\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/unet\"\n",
    "        )\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/text_encoder\"\n",
    "        )\n",
    "\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            model_id, unet=unet, text_encoder=text_encoder, torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_id = OUTPUT_DIR\n",
    "\n",
    "        print(f\"Model = {model_id}\")\n",
    "\n",
    "        image_dir = OUTPUT_DIR + \"/test_output_images\"\n",
    "\n",
    "        if image_dir is not None:\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    # pipeline.to(\"cuda\")\n",
    "    # pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "    # pipeline = pipeline.to(accelerator.device)\n",
    "    # pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # Perform inference, or save, or push to the hub\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt = {prompt}\")\n",
    "\n",
    "        images = pipeline(\n",
    "            prompt,\n",
    "            # negative_prompt=\"deformed\",\n",
    "            num_inference_steps=20,\n",
    "            # num_inference_steps=40,\n",
    "            # num_inference_steps=100,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_images_per_prompt=2,\n",
    "            # num_images_per_prompt=4,\n",
    "        ).images\n",
    "\n",
    "        for img in images:\n",
    "            now = datetime.datetime.isoformat(datetime.datetime.today())\n",
    "\n",
    "            display(img)\n",
    "\n",
    "            img.save(image_dir + \"/\" + now + \".png\")\n",
    "\n",
    "        # pipeline.save_pretrained(\"dreambooth-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Free runtime memory\n",
    "exit()?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2"
   },
   "outputs": [],
   "source": [
    "# @markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    num_samples,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "            prompt,\n",
    "            height=int(height),\n",
    "            width=int(width),\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=int(num_samples),\n",
    "            num_inference_steps=int(num_inference_steps),\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda,\n",
    "        ).images\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(\n",
    "        inference,\n",
    "        inputs=[\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            height,\n",
    "            width,\n",
    "            num_inference_steps,\n",
    "            guidance_scale,\n",
    "        ],\n",
    "        outputs=gallery,\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
