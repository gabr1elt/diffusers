{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd /root/Development/examples/dreambooth\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "source venv/bin/activate\n",
    "\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install -e .\n",
    "cd ..\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "pip install safetensors\n",
    "pip install torch-tb-profiler\n",
    "pip install compel\n",
    "\n",
    "git lfs install\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# python3 -c \"from accelerate.utils import write_basic_config; write_basic_config()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/CompVis/stable-diffusion-v1-4 training_models/stable-diffusion-v1-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/runwayml/stable-diffusion-v1-55588 training_models/stable-diffusion-v1-555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/stabilityai/stable-diffusion-2-1-base training_models/stable-diffusion-2-1-base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person_ddim training_images/Stable-Diffusion-Regularization-Images-person_ddim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_1_ddim_step training_images/Stable-Diffusion-Regularization-Images-man_1_ddim_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_euler training_images/Stable-Diffusion-Regularization-Images-man_euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person-photographs training_images/Stable-Diffusion-Regularization-Images-person-photographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./dog\"\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=local_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.19.0\n",
      "- Platform: Linux-6.1.0-9-amd64-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.6\n",
      "- Numpy version: 1.24.3\n",
      "- PyTorch version (GPU?): 2.0.1+cu117 (False)\n",
      "- System RAM: 61.93 GB\n",
      "- `Accelerate` default config:\n",
      "\t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: NO\n",
      "\t- mixed_precision: no\n",
      "\t- use_cpu: False\n",
      "\t- num_processes: 1\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- gpu_ids: all\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: True\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: no\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n",
      "\t- tpu_env: []\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"CompVis/stable-diffusion-v1-4\"\n",
    "# MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "# MODEL_NAME = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "# MODEL_NAME = \"stabilityai/stable-diffusion-2-1\"\n",
    "# MODEL_NAME = \"BAAI/AltDiffusion-m9\"\n",
    "# MODEL_NAME = \"BAAI/AltDiffusion\"\n",
    "\n",
    "# MODEL_NAME = \"stable-diffusion-v1-5\"\n",
    "MODEL_NAME = \"stable-diffusion-2-1-base\"\n",
    "MODEL_PATH = f\"training_models/{MODEL_NAME}\"\n",
    "\n",
    "# INSTANCE_NAME = \"dog\"\n",
    "# INSTANCE_NAME = \"rabbit_toy\"\n",
    "# INSTANCE_NAME = \"gabrieltorcat\"\n",
    "# INSTANCE_NAME = \"gabrieltorcat2\"\n",
    "INSTANCE_NAME = \"gabrieltorcat_512\"\n",
    "INSTANCE_DIR = f\"training_images/{INSTANCE_NAME}\"\n",
    "# CLASS_DIR = \"dog\"\n",
    "# CLASS_DIR = \"toy\"\n",
    "CLASS_DIR = \"man\"\n",
    "# CLASS_DIR = \"person\"\n",
    "# CLASS_DIR = \"Stable-Diffusion-Regularization-Images-man_1_ddim_step/man_1_ddim_step\"\n",
    "# CLASS_DIR = \"Stable-Diffusion-Regularization-Images-person_ddim/person_ddim\"\n",
    "CLASS_DIR = f\"regularization_images/{MODEL_NAME}/{CLASS_DIR}\"\n",
    "OUTPUT_DIR = f\"outputs/{MODEL_NAME}/{INSTANCE_NAME}\"\n",
    "\n",
    "# PROMPT_TOKEN = \"sks\"\n",
    "# PROMPT_TOKEN = \"dbDog\"\n",
    "# PROMPT_TOKEN = \"dbRabbit\"\n",
    "PROMPT_TOKEN = \"gabrieltorcat\"\n",
    "# CLASS_TOKEN = \"dog\"\n",
    "# CLASS_TOKEN = \"toy\"\n",
    "CLASS_TOKEN = \"man\"\n",
    "# CLASS_TOKEN = \"person\"\n",
    "INSTANCE_PROMPT = f\"a photo of {PROMPT_TOKEN} {CLASS_TOKEN}\"\n",
    "# VALIDATION_PROMPT = f\"an oil painting of {PROMPT_TOKEN} {CLASS_TOKEN} sitting next to a wooden window reading a book, by Vincent Van Gogh\"\n",
    "VALIDATION_PROMPT = (\n",
    "    # f\"an oil painting of {PROMPT_TOKEN} {CLASS_TOKEN}, by Vincent Van Gogh\"\n",
    "    f\"a photo of {PROMPT_TOKEN} {CLASS_TOKEN} riding a bike in New York city\"\n",
    ")\n",
    "CLASS_PROMPT = f\"a photo of {CLASS_TOKEN}\"\n",
    "\n",
    "RESOLUTION = 512\n",
    "# RESOLUTION = 768\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "# TRAIN_BATCH_SIZE = 2\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "# GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "# LEARNING_RATE = 5e-6\n",
    "# LEARNING_RATE = 2e-6\n",
    "# LEARNING_RATE = 1e-6\n",
    "# LEARNING_RATE = 9e-7\n",
    "LEARNING_RATE = 8e-7\n",
    "\n",
    "# MAX_TRAIN_STEPS = 400\n",
    "# MAX_TRAIN_STEPS = 800\n",
    "# MAX_TRAIN_STEPS = 1200\n",
    "MAX_TRAIN_STEPS = 1600\n",
    "\n",
    "# NUM_CLASS_IMAGES = 50\n",
    "# NUM_CLASS_IMAGES = 100\n",
    "# NUM_CLASS_IMAGES = 300\n",
    "# NUM_CLASS_IMAGES = 500\n",
    "NUM_CLASS_IMAGES = 1500\n",
    "SAMPLE_BATCH_SIZE = 1\n",
    "# SAMPLE_BATCH_SIZE = 2\n",
    "\n",
    "CHECKPOINTING_STEPS = 200\n",
    "# CHECKPOINTING_STEPS = 300\n",
    "# CHECKPOINTING_STEPS = 400\n",
    "\n",
    "HUB_TOKEN = \"\"\n",
    "\n",
    "# NUM_CPU_THREADS_PER_PROCESS = 16\n",
    "# NUM_CPU_THREADS_PER_PROCESS = 20\n",
    "NUM_CPU_THREADS_PER_PROCESS = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "accelerate launch \\\n",
    "  \n",
    "  --num_cpu_threads_per_process=$NUM_CPU_THREADS_PER_PROCESS \\\n",
    "  \n",
    "  train_dreambooth.py \\\n",
    "  \n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\" \\\n",
    "\n",
    "  --train_text_encoder \\\n",
    "\n",
    "# bitsandbytes\n",
    "#   --use_8bit_adam \\\n",
    "#   --gradient_checkpointing \\\n",
    "\n",
    "# xformers\n",
    "#   --enable_xformers_memory_efficient_attention \\\n",
    "#   --set_grads_to_none \\\n",
    "\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  # --resume_from_checkpoint=\"checkpoint-1500\" \\\n",
    "  --resume_from_checkpoint=\"latest\" \\\n",
    "\n",
    "  # --push_to_hub \\\n",
    "  # --hub_token=$HUB_TOKEN \\\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "#####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# %env LD_LIBRARY_PATH=/usr/lib/x86_64_linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/accelerate/accelerator.py:258: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "05/28/2023 15:54:37 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'time_embedding_dim', 'resnet_out_scale_factor', 'time_cond_proj_dim', 'class_embeddings_concat', 'mid_block_type', 'timestep_post_act', 'class_embed_type', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'encoder_hid_dim', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'upcast_attention', 'resnet_skip_time_act', 'addition_embed_type', 'time_embedding_type', 'conv_out_kernel', 'time_embedding_act_fn', 'cross_attention_norm', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
      "/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "05/28/2023 15:54:37 - INFO - __main__ - Number of class images to sample: 1000.\n",
      "Generating class images:   0%|                         | 0/1000 [00:00<?, ?it/s]^C\n",
      "Generating class images:   0%|                         | 0/1000 [00:20<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/Development/examples/dreambooth/train_dreambooth.py\", line 1085, in <module>\n",
      "    main(args)\n",
      "  File \"/root/Development/examples/dreambooth/train_dreambooth.py\", line 691, in main\n",
      "    images = pipeline(example[\"prompt\"]).images\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\", line 684, in __call__\n",
      "    noise_pred = self.unet(\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/models/unet_2d_condition.py\", line 773, in forward\n",
      "    sample = upsample_block(\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/models/unet_2d_blocks.py\", line 1860, in forward\n",
      "    hidden_states = attn(\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/models/transformer_2d.py\", line 265, in forward\n",
      "    hidden_states = block(\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/models/attention.py\", line 347, in forward\n",
      "    ff_output = self.ff(norm_hidden_states)\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/diffusers/src/diffusers/models/attention.py\", line 405, in forward\n",
      "    hidden_states = module(hidden_states)\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/Development/examples/dreambooth/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch \\\n",
    "  --num_cpu_threads_per_process=$NUM_CPU_THREADS_PER_PROCESS \\\n",
    "  train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\" \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  --resume_from_checkpoint=\"latest\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to stable diffusion format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python ../../scripts/convert_diffusers_to_original_stable_diffusion.py \\\n",
    "    --model_path=\"$OUTPUT_DIR\" \\\n",
    "    --checkpoint_path=\"$OUTPUT_DIR/\"$MODEL_NAME\"_\"$PROMPT_TOKEN\"_\"$CLASS_TOKEN\".ckpt\" \\\n",
    "    --half \\\n",
    "    # --use_safetensors \\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir OUTPUT_DIR + \"/logs/dreambooth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir $OUTPUT_DIR\"/logs/dreambooth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cinematic photo, highly detailed, cinematic lighting, ultra-detailed, ultrarealistic, photorealism, Octane Rendering, cyberpunk lights, Hyper Detail, 8K, HD, Unreal Engine, V-Ray, full hd, cyberpunk, abstract, 3d octane render + 4k UHD + immense detail + dramatic lighting + well lit + black, purple, blue, pink, cerulean, teal, metallic colours, + fine details, ultra photoreal, photographic, concept art, cinematic composition, rule of thirds, mysterious, eerie, photorealism, breathtaking detailed, painting art deco pattern, by hsiao, ron cheng, john james audubon, bizarre compositions, exquisite detail, extremely moody lighting, painted by greg rutkowski makoto shinkai takashi takeuchi studio ghibli, akihiko yoshida'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cinematic photo, highly detailed, cinematic lighting, ultra-detailed, ultrarealistic, photorealism, Octane Rendering, cyberpunk lights, Hyper Detail, 8K, HD, Unreal Engine, V-Ray, full hd, cyberpunk, abstract, 3d octane render + 4k UHD + immense detail + dramatic lighting + well lit + black, purple, blue, pink, cerulean, teal, metallic colours, + fine details, ultra photoreal, photographic, concept art, cinematic composition, rule of thirds, mysterious, eerie, photorealism, breathtaking detailed, painting art deco pattern, by hsiao, ron cheng, john james audubon, bizarre compositions, exquisite detail, extremely moody lighting, painted by greg rutkowski makoto shinkai takashi takeuchi studio ghibli, akihiko yoshida\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA_PROMPT = \", intricate, highly detailed, smooth, sharp focus, detailed clothing, detailed face, hyperrealistic, cinematic lighting, high resolution, photorealistic, masterpiece, 4K, 8K\"\n",
    "EXTRA_PROMPT = \", intricate, highly detailed, smooth, sharp focus, detailed clothing, detailed face, hyperrealistic, high resolution, photorealistic, masterpiece, 4K, 8K\"\n",
    "\n",
    "prompts = [\n",
    "    INSTANCE_PROMPT,\n",
    "    VALIDATION_PROMPT,\n",
    "    f\"oil painting of {PROMPT_TOKEN} {CLASS_TOKEN}\",\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} on top of mount fuji\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} (shaking hands)+ with barack obama\" + EXTRA_PROMPT,\n",
    "    f\"({PROMPT_TOKEN} {CLASS_TOKEN})+ close to barack obama\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} in front of the eiffel tower\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling as an astronaut\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} wearing an astronaut suit in space++\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling floating++ inside the international space station\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a cyborg\" + EXTRA_PROMPT,\n",
    "    f\"digital painting of {PROMPT_TOKEN} {CLASS_TOKEN} as a very bearded pirate with a hat, cinematic lighting, artstation, concept art, illustration, artgerm, bouguereau, fantasy, Surrealist\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a cyborg, full lenght shot, super hero pose, biomechanical suit, inflateble shapes, wearing epic bionic cyborg implants, biopunk futuristic wardrobe, artstation, concept art, cyberpunk, octane render\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} shaking hands with (emmanuel macron)0.8\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling close to (emmanuel macron)0.8\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling close to a beautiful (Jennifer Lawrence)++ at a party\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling close to a beautiful (Jennifer Lawrence)++ on the red carpet at the oscars\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} in a ramen bowl\" + EXTRA_PROMPT,\n",
    "    f\"oil painting of {PROMPT_TOKEN} {CLASS_TOKEN}, by Picasso\",\n",
    "    f\"oil painting of ({PROMPT_TOKEN} {CLASS_TOKEN})++++, by Van Gogh\",\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a character of the movie (the matrix)++ fighting morpheus from the movie (the matrix)++\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} doing a handstand\" + EXTRA_PROMPT,\n",
    "    f\"a plastic toy of {PROMPT_TOKEN} {CLASS_TOKEN}\",\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a fighter jet pilot\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a F22++ pilot\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a pilot inside the cockpit of an air force fighter jet\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} piloting a fighter jet\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} piloting an F22++\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling close to a young++ beautiful Angelina Jolie\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling close to a young++ beautiful Angelina Jolie on the red carpet at the oscars\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} by the ocean\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} celebrating and holding the FIFA football world cup surrounded by famous footballers\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} boxing against Mike Tyson, angry, muscular\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} with a (face tatoo)+ close to Mike Tyson\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} driving formula 1 car, championship\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} winning the super bowl\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as mad max from mad max fury road\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} sitting close to mahatma gandhi+, (old photo)++\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as the joker from the dark night\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as the terminator\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a (blue Na'vi)++ from the (movie Avatar)++ (photo realistic)+++, cgi, cinema\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a hobbit\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} smiling as an olympic athlete\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} holding an (olympic gold medal)+ at the podium, celebration, stadium, press\"\n",
    "    + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a cowboy\" + EXTRA_PROMPT,\n",
    "    f\"{PROMPT_TOKEN} {CLASS_TOKEN} as a sheriff riding a horse in the (wild west)++, old photo\" + EXTRA_PROMPT,\n",
    "    # \"autumn in paris, ornate, beautiful, atmosphere, vibe, mist, smoke, fire, chimney, rain, wet, pristine, puddles, melting, dripping, snow, creek, lush, ice, bridge, forest, roses, flowers, by stanley artgerm lau, greg rutkowski, thomas kindkade, alphonse mucha, loish, norman rockwell\",\n",
    "    # \"emma watson as nature magic celestial, top down pose, long hair, soft pink and white transparent cloth, space, D&D, shiny background, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, artgerm, bouguereau\",\n",
    "    # \"Emma Watson as a powerful mysterious sorceress, casting lightning magic, detailed clothing, digital painting, hyperrealistic, fantasy, Surrealist, full body, by Stanley Artgerm Lau and Alphonse Mucha, artstation, highly detailed, sharp focus, sci-fi, stunningly beautiful, dystopian, iridescent gold, cinematic lighting, dark\",\n",
    "    # \"full lenght shot, super hero pose, biomechanical suit, inflateble shapes, wearing epic bionic cyborg implants, masterpiece, intricate, biopunk futuristic wardrobe, highly detailed, artstation, concept art, cyberpunk, octane render\",\n",
    "    # \"\",\n",
    "    # \"\",\n",
    "]\n",
    "\n",
    "# NEGATIVE_PROMPT = None\n",
    "NEGATIVE_PROMPT = \"ugly, distorted face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, blurry, blurred, grainy, draft, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face,\"\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from diffusers import (\n",
    "    UNet2DConditionModel,\n",
    "    # DiffusionPipeline,\n",
    "    StableDiffusionPipeline,\n",
    "    KDPM2DiscreteScheduler,\n",
    "    DDIMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "from compel import Compel\n",
    "from IPython.display import display\n",
    "\n",
    "CHECKPOINTS = [\n",
    "    # 200,\n",
    "    # 400,\n",
    "    # 600,\n",
    "    # 800,\n",
    "    # 1000,\n",
    "    # 1200,\n",
    "    # 1400,\n",
    "    # 1600,\n",
    "    None,\n",
    "]\n",
    "\n",
    "# HEIGHT=384\n",
    "HEIGHT = 512\n",
    "\n",
    "# WIDTH=512\n",
    "WIDTH = 640\n",
    "# WIDTH=768\n",
    "\n",
    "# NUM_INFERENCE_STEPS=20\n",
    "# NUM_INFERENCE_STEPS=40\n",
    "# NUM_INFERENCE_STEPS=60\n",
    "# NUM_INFERENCE_STEPS=70\n",
    "# NUM_INFERENCE_STEPS=80\n",
    "NUM_INFERENCE_STEPS = 100\n",
    "\n",
    "# GUIDANCE_SCALE=7.5\n",
    "GUIDANCE_SCALE = 10\n",
    "# GUIDANCE_SCALE=15\n",
    "\n",
    "# NUM_IMAGES_PER_PROMPT = 1\n",
    "# NUM_IMAGES_PER_PROMPT=2\n",
    "# NUM_IMAGES_PER_PROMPT=3\n",
    "# NUM_IMAGES_PER_PROMPT=4\n",
    "NUM_IMAGES_PER_PROMPT = 6\n",
    "\n",
    "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
    "model_id = MODEL_PATH\n",
    "\n",
    "print(f\"Model = {model_id}\")\n",
    "\n",
    "# loop over checkpoints and final model\n",
    "for checkpoint in CHECKPOINTS:\n",
    "    print(f\"Checkpoint = {checkpoint}\")\n",
    "\n",
    "    # if is checkpoint\n",
    "    if checkpoint:\n",
    "        image_dir = OUTPUT_DIR + \"/test_output_images\" + \"/checkpoint-\" + str(checkpoint)\n",
    "\n",
    "        unet = UNet2DConditionModel.from_pretrained(OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/unet\")\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/text_encoder\")\n",
    "\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, unet=unet, text_encoder=text_encoder, torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    # if is final model\n",
    "    else:\n",
    "        model_id = OUTPUT_DIR\n",
    "\n",
    "        print(f\"Model = {model_id}\")\n",
    "\n",
    "        image_dir = OUTPUT_DIR + \"/test_output_images\"\n",
    "\n",
    "        # pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "\n",
    "    pipeline.to(\"cuda\")\n",
    "    # pipeline = pipeline.to(accelerator.device)\n",
    "    # pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # set scheduler\n",
    "    # pipeline.scheduler = KDPM2DiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "    # pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "    # pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
    "    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "    # prompt weighting\n",
    "    compel_proc = Compel(tokenizer=pipeline.tokenizer, text_encoder=pipeline.text_encoder)\n",
    "\n",
    "    # create output dir\n",
    "    if image_dir is not None:\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    # Perform inference, or save, or push to the hub\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt = {prompt}\")\n",
    "\n",
    "        # create promp embeding\n",
    "        prompt_embeds = compel_proc(prompt)\n",
    "\n",
    "        images = pipeline(\n",
    "            # prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt=NEGATIVE_PROMPT,\n",
    "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH,\n",
    "            num_images_per_prompt=NUM_IMAGES_PER_PROMPT,\n",
    "        ).images\n",
    "\n",
    "        for img in images:\n",
    "            now = datetime.datetime.isoformat(datetime.datetime.today())\n",
    "\n",
    "            # display(img)\n",
    "\n",
    "            img.save(image_dir + \"/\" + now + \".png\")\n",
    "\n",
    "        # pipeline.save_pretrained(\"dreambooth-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Free runtime memory\n",
    "exit()?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2"
   },
   "outputs": [],
   "source": [
    "# @markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    num_samples,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "            prompt,\n",
    "            height=int(height),\n",
    "            width=int(width),\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=int(num_samples),\n",
    "            num_inference_steps=int(num_inference_steps),\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda,\n",
    "        ).images\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(\n",
    "        inference,\n",
    "        inputs=[\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            height,\n",
    "            width,\n",
    "            num_inference_steps,\n",
    "            guidance_scale,\n",
    "        ],\n",
    "        outputs=gallery,\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
