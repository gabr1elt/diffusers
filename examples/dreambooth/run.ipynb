{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd /home/user/Development/diffusers/examples/dreambooth\n",
    "\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install notebook\n",
    "\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install -e .\n",
    "cd ..\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "accelerate config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git lfs install\n",
    "\n",
    "git clone https://huggingface.co/CompVis/stable-diffusion-v1-4\n",
    "git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
    "git clone https://huggingface.co/stabilityai/stable-diffusion-2-1-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person_ddim.git\n",
    "# git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_1_ddim_step.git\n",
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-man_euler\n",
    "git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-person-photographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./dog\"\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=local_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "# MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "# MODEL_NAME=\"stabilityai/stable-diffusion-2-1-base\"\n",
    "# MODEL_NAME=\"stabilityai/stable-diffusion-2-1\"\n",
    "# MODEL_NAME=\"BAAI/AltDiffusion-m9\"\n",
    "# MODEL_NAME=\"BAAI/AltDiffusion\"\n",
    "\n",
    "# MODEL_NAME=\"stable-diffusion-v1-5\"\n",
    "MODEL_NAME=\"stable-diffusion-2-1-base\"\n",
    "MODEL_PATH=\"training_models/\"$MODEL_NAME\n",
    "\n",
    "# INSTANCE_NAME=\"dog\"\n",
    "# INSTANCE_NAME=\"rabbit_toy\"\n",
    "# INSTANCE_NAME=\"gabrieltorcat\"\n",
    "# INSTANCE_NAME=\"gabrieltorcat2\"\n",
    "INSTANCE_NAME=\"gabrieltorcat_512\"\n",
    "INSTANCE_DIR=\"training_images/\"$INSTANCE_NAME\n",
    "# CLASS_DIR=\"dog\"\n",
    "# CLASS_DIR=\"toy\"\n",
    "CLASS_DIR=\"man\"\n",
    "# CLASS_DIR=\"person\"\n",
    "# CLASS_DIR=\"Stable-Diffusion-Regularization-Images-man_1_ddim_step/man_1_ddim_step\"\n",
    "# CLASS_DIR=\"Stable-Diffusion-Regularization-Images-person_ddim/person_ddim\"\n",
    "CLASS_DIR=\"regularization_images/\"$MODEL_NAME\"/\"$CLASS_DIR\n",
    "OUTPUT_DIR=$INSTANCE_NAME\n",
    "OUTPUT_DIR=\"outputs/\"$MODEL_NAME\"/\"$OUTPUT_DIR\n",
    "\n",
    "# PROMPT_TOKEN=\"sks\"\n",
    "# PROMPT_TOKEN=\"dbDog\"\n",
    "# PROMPT_TOKEN=\"dbRabbit\"\n",
    "PROMPT_TOKEN=\"gabrieltorcat\"\n",
    "# CLASS_TOKEN=\"dog\"\n",
    "# CLASS_TOKEN=\"toy\"\n",
    "CLASS_TOKEN=\"man\"\n",
    "# CLASS_TOKEN=\"person\"\n",
    "INSTANCE_PROMPT=\"a photo of $PROMPT_TOKEN $CLASS_TOKEN\"\n",
    "# VALIDATION_PROMPT=\"an oil painting of $PROMPT_TOKEN $CLASS_TOKEN sitting next to a wooden window reading a book, by Vincent Van Gogh\"\n",
    "VALIDATION_PROMPT=\"an oil painting of $PROMPT_TOKEN $CLASS_TOKEN, by Vincent Van Gogh\"\n",
    "CLASS_PROMPT=\"a photo of $CLASS_TOKEN\"\n",
    "\n",
    "RESOLUTION=512\n",
    "# RESOLUTION=768\n",
    "TRAIN_BATCH_SIZE=1\n",
    "# TRAIN_BATCH_SIZE=2\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS=1\n",
    "# GRADIENT_ACCUMULATION_STEPS=2\n",
    "\n",
    "# LEARNING_RATE=5e-6\n",
    "# LEARNING_RATE=2e-6\n",
    "# LEARNING_RATE=1e-6\n",
    "LEARNING_RATE=9e-7\n",
    "\n",
    "# MAX_TRAIN_STEPS=400\n",
    "# MAX_TRAIN_STEPS=800\n",
    "# MAX_TRAIN_STEPS=1200\n",
    "MAX_TRAIN_STEPS=1600\n",
    "\n",
    "# NUM_CLASS_IMAGES=50\n",
    "# NUM_CLASS_IMAGES=100\n",
    "# NUM_CLASS_IMAGES=300\n",
    "NUM_CLASS_IMAGES=500\n",
    "# NUM_CLASS_IMAGES=1500\n",
    "SAMPLE_BATCH_SIZE=1\n",
    "# SAMPLE_BATCH_SIZE=2\n",
    "\n",
    "CHECKPOINTING_STEPS=200\n",
    "# CHECKPOINTING_STEPS=400\n",
    "\n",
    "HUB_TOKEN=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# test vars\n",
    "echo $MODEL_NAME\n",
    "echo $INSTANCE_NAME\n",
    "echo $VALIDATION_PROMPT\n",
    "echo $MAX_TRAIN_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\" \\\n",
    "\n",
    "  --train_text_encoder \\\n",
    "\n",
    "# bitsandbytes\n",
    "#   --use_8bit_adam \\\n",
    "#   --gradient_checkpointing \\\n",
    "\n",
    "# xformers\n",
    "#   --enable_xformers_memory_efficient_attention \\\n",
    "#   --set_grads_to_none \\\n",
    "\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  # --resume_from_checkpoint=\"checkpoint-1500\" \\\n",
    "  --resume_from_checkpoint=\"latest\" \\\n",
    "\n",
    "  # --push_to_hub \\\n",
    "  # --hub_token=$HUB_TOKEN \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"$MODEL_PATH\"  \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "  --class_prompt=\"$CLASS_PROMPT\" \\\n",
    "  --instance_prompt=\"$INSTANCE_PROMPT\" \\\n",
    "  --validation_prompt=\"$VALIDATION_PROMPT\" \\\n",
    "  --resolution=$RESOLUTION \\\n",
    "  --train_batch_size=$TRAIN_BATCH_SIZE \\\n",
    "  --gradient_accumulation_steps=$GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=$SAMPLE_BATCH_SIZE \\\n",
    "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "  --resume_from_checkpoint=\"latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir $OUTPUT_DIR\"/logs/dreambooth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"stable-diffusion-v1-5\"\n",
    "MODEL_NAME = \"stable-diffusion-2-1-base\"\n",
    "MODEL_PATH = \"training_models/\" + MODEL_NAME\n",
    "\n",
    "# INSTANCE_NAME=\"dog\"\n",
    "# INSTANCE_NAME=\"rabbit_toy\"\n",
    "# INSTANCE_NAME=\"gabrieltorcat\"\n",
    "# INSTANCE_NAME = \"gabrieltorcat2\"\n",
    "INSTANCE_NAME = \"gabrieltorcat_512\"\n",
    "INSTANCE_DIR = \"training_images/\" + INSTANCE_NAME\n",
    "# CLASS_DIR=\"dog\"\n",
    "# CLASS_DIR=\"toy\"\n",
    "CLASS_DIR = \"man\"\n",
    "# CLASS_DIR=\"person\"\n",
    "# CLASS_DIR=\"Stable-Diffusion-Regularization-Images-man_1_ddim_step/man_1_ddim_step\"\n",
    "# CLASS_DIR=\"Stable-Diffusion-Regularization-Images-person_ddim/person_ddim\"\n",
    "CLASS_DIR = \"regularization_images/\" + MODEL_NAME + \"/\" + CLASS_DIR\n",
    "OUTPUT_DIR = INSTANCE_NAME\n",
    "OUTPUT_DIR = \"outputs/\" + MODEL_NAME + \"/\" + OUTPUT_DIR\n",
    "\n",
    "# PROMPT_TOKEN=\"sks\"\n",
    "# PROMPT_TOKEN=\"dbDog\"\n",
    "# PROMPT_TOKEN=\"dbRabbit\"\n",
    "PROMPT_TOKEN = \"gabrieltorcat\"\n",
    "# CLASS_TOKEN=\"dog\"\n",
    "# CLASS_TOKEN=\"toy\"\n",
    "CLASS_TOKEN = \"man\"\n",
    "# CLASS_TOKEN=\"person\"\n",
    "INSTANCE_PROMPT = \"a photo of \" + PROMPT_TOKEN + \" \" + CLASS_TOKEN\n",
    "VALIDATION_PROMPT = (\n",
    "    \"an oil painting of \"\n",
    "    + PROMPT_TOKEN\n",
    "    + \" \"\n",
    "    + CLASS_TOKEN\n",
    "    # + \" sitting next to a wooden window reading a book, by Vincent Van Gogh\"\n",
    "    + \", by Vincent Van Gogh\"\n",
    ")\n",
    "CLASS_PROMPT = \"a photo of \" + CLASS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from diffusers import (\n",
    "    DiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    StableDiffusionPipeline,\n",
    "    DDIMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# prompts = [INSTANCE_PROMPT]\n",
    "# prompts = [VALIDATION_PROMPT]\n",
    "# prompts = [INSTANCE_PROMPT, VALIDATION_PROMPT]\n",
    "prompts = [\n",
    "    INSTANCE_PROMPT + \" riding a bike\",\n",
    "    INSTANCE_PROMPT + \" on top of mount fuji\",\n",
    "    INSTANCE_PROMPT + \" in front of the eiffel tower\",\n",
    "    INSTANCE_PROMPT + \" mecha robot\",\n",
    "    INSTANCE_PROMPT + \" in a ramen bowl\",\n",
    "    INSTANCE_PROMPT + \" by the ocean\",\n",
    "    INSTANCE_PROMPT + \" as mad max from mad max fury road\",\n",
    "    INSTANCE_PROMPT + \" as the joker from the dark night\",\n",
    "    INSTANCE_PROMPT + \" as the terminator\",\n",
    "    INSTANCE_PROMPT + \" as a hobbit\",\n",
    "    INSTANCE_PROMPT + \" as an astronaut\",\n",
    "    INSTANCE_PROMPT + \" shaking hands with barack obama\",\n",
    "    INSTANCE_PROMPT + \" shaking hands with emmanuel macron\",\n",
    "    INSTANCE_PROMPT + \" as an olympic athlete\",\n",
    "    INSTANCE_PROMPT + \" as a pirate\",\n",
    "    INSTANCE_PROMPT + \" as a fighter jet pilot\",\n",
    "]\n",
    "\n",
    "CHECKPOINTS = [\n",
    "    # 200,\n",
    "    # 400,\n",
    "    # 600,\n",
    "    # 800,\n",
    "    # 1000,\n",
    "    # 1200,\n",
    "    # 1600,\n",
    "    None,\n",
    "]\n",
    "\n",
    "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
    "model_id = MODEL_PATH\n",
    "\n",
    "print(f\"Model = {model_id}\")\n",
    "\n",
    "for checkpoint in CHECKPOINTS:\n",
    "    print(f\"Checkpoint = {checkpoint}\")\n",
    "\n",
    "    if checkpoint:\n",
    "        image_dir = (\n",
    "            OUTPUT_DIR + \"/test_output_images\" + \"/checkpoint-\" + str(checkpoint)\n",
    "        )\n",
    "\n",
    "        if image_dir is not None:\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/unet\"\n",
    "        )\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            OUTPUT_DIR + \"/checkpoint-\" + str(checkpoint) + \"/text_encoder\"\n",
    "        )\n",
    "\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            model_id, unet=unet, text_encoder=text_encoder, torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_id = OUTPUT_DIR\n",
    "\n",
    "        print(f\"Model = {model_id}\")\n",
    "\n",
    "        image_dir = OUTPUT_DIR + \"/test_output_images\"\n",
    "\n",
    "        if image_dir is not None:\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    # pipeline.to(\"cuda\")\n",
    "    # pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "    # pipeline = pipeline.to(accelerator.device)\n",
    "    # pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # Perform inference, or save, or push to the hub\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt = {prompt}\")\n",
    "\n",
    "        images = pipeline(\n",
    "            prompt,\n",
    "            # negative_prompt=\"deformed\",\n",
    "            # num_inference_steps=20,\n",
    "            num_inference_steps=40,\n",
    "            # num_inference_steps=100,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            # num_images_per_prompt=2,\n",
    "        ).images\n",
    "\n",
    "        for img in images:\n",
    "            now = datetime.datetime.isoformat(datetime.datetime.today())\n",
    "\n",
    "            display(img)\n",
    "\n",
    "            img.save(image_dir + \"/\" + now + \".png\")\n",
    "\n",
    "        # pipeline.save_pretrained(\"dreambooth-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [],
   "source": [
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"photo of zwx dog\" \\\n",
    "  --concepts_list=\"concepts_list.json\"\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "# @markdown Specify the weights directory to use (leave blank for latest)\n",
    "WEIGHTS_DIR = \"\"  # @param {type:\"string\"}\n",
    "if WEIGHTS_DIR == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "\n",
    "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "# @markdown Run to generate a grid of preview images from the last saved weights.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted(\n",
    "    [f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x)\n",
    ")\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(\n",
    "    row, col, figsize=(col * scale, row * scale), gridspec_kw={\"hspace\": 0, \"wspace\": 0}\n",
    ")\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(\n",
    "                -0.1,\n",
    "                0.5,\n",
    "                folder,\n",
    "                rotation=0,\n",
    "                va=\"center\",\n",
    "                ha=\"center\",\n",
    "                transform=currAxes.transAxes,\n",
    "            )\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap=\"gray\")\n",
    "        currAxes.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grid.png\", dpi=72)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dcXzsUyG1aCy"
   },
   "outputs": [],
   "source": [
    "#@markdown Run conversion.\n",
    "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True #@param {type: \"boolean\"}\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2"
   },
   "outputs": [],
   "source": [
    "# @markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    num_samples,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "            prompt,\n",
    "            height=int(height),\n",
    "            width=int(width),\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=int(num_samples),\n",
    "            num_inference_steps=int(num_inference_steps),\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda,\n",
    "        ).images\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(\n",
    "        inference,\n",
    "        inputs=[\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            height,\n",
    "            width,\n",
    "            num_inference_steps,\n",
    "            guidance_scale,\n",
    "        ],\n",
    "        outputs=gallery,\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lJoOgLQHnC8L"
   },
   "outputs": [],
   "source": [
    "# @title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
    "\n",
    "# @markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
    "import shutil\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "for f in glob(OUTPUT_DIR + os.sep + \"*\"):\n",
    "    if f != WEIGHTS_DIR:\n",
    "        shutil.rmtree(f)\n",
    "        print(\"Deleted\", f)\n",
    "for f in glob(WEIGHTS_DIR + \"/*\"):\n",
    "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
    "        try:\n",
    "            shutil.rmtree(f)\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "        print(\"Deleted\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXgi8HM4c-DA"
   },
   "outputs": [],
   "source": [
    "# @title Free runtime memory\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
